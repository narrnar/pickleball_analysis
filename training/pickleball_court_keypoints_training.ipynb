{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a1d467",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35691cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(68915) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /opt/anaconda3/lib/python3.12/site-packages (1.2.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2025.1.31)\n",
      "Requirement already satisfied: idna==3.7 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.4.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (11.3.0)\n",
      "Requirement already satisfied: pi-heif<2 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.21.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (4.51.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->roboflow) (3.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "# Get roboflow data for pickleball court keypoints detection training\n",
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"KEYHERE\")\n",
    "# project = rf.workspace(\"pickleball-ball-detection\").project(\"pickleball-court-keypoints-syncz\")\n",
    "# version = project.version(6)\n",
    "# dataset = version.download(\"coco\")\n",
    "\n",
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"KEYHERE\")\n",
    "project = rf.workspace(\"pickleball-mskux\").project(\"court-detection-bxo2j-npaqa\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bc27f",
   "metadata": {},
   "source": [
    "# Start Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f640135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1290d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267268dc",
   "metadata": {},
   "source": [
    "# Create Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d51b5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class keyPointsDataset(Dataset):\n",
    "    def __init__(self, img_dir, data_file):\n",
    "        self.img_dir = img_dir\n",
    "        with open(data_file, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(img)\n",
    "        kps = np.array(item['kps']).flatten()\n",
    "        kps = kps.astype(np.float32)\n",
    "\n",
    "        # Adjust keypoints\n",
    "        kps[::2] *= (224 / w) # Adjust x coordinates\n",
    "        kps[1::2] *= (224 / h) # Adjust y coordinates\n",
    "\n",
    "        return img, kps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63767416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAT train: 568 items -> data_idx/data_train_flat.json, images in data_idx/images\n",
      "FLAT val:   0 items -> data_idx/data_val_flat.json\n"
     ]
    }
   ],
   "source": [
    "# --- YOLOv8 -> flat adapter for your keyPointsDataset (no changes to your dataset class) ---\n",
    "from pathlib import Path\n",
    "import json, cv2\n",
    "\n",
    "# Point to the YOLOv8 export directories from Roboflow\n",
    "root = Path(\"Court-Detection-1\")  # adjust if your folder name differs\n",
    "train_dir = root / \"train\"\n",
    "val_dir   = root / (\"valid\" if (root / \"valid\").exists() else \"val\")\n",
    "\n",
    "rf_train_images = train_dir / \"images\"\n",
    "rf_train_labels = train_dir / \"labels\"\n",
    "rf_val_images   = val_dir / \"images\"\n",
    "rf_val_labels   = val_dir / \"labels\"\n",
    "\n",
    "# Output (what your dataset will consume)\n",
    "out_img_dir   = Path(\"data_idx/images\")\n",
    "out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_train_json = Path(\"data_idx/data_train_flat.json\")\n",
    "out_val_json   = Path(\"data_idx/data_val_flat.json\")\n",
    "\n",
    "def parse_yolo_kps(img_dir: Path, lbl_dir: Path, out_img_dir: Path, start_idx=0, num_kps=12):\n",
    "    \"\"\"\n",
    "    Reads YOLOv8 keypoints labels and writes:\n",
    "      - images: data_idx/images/{id}.png\n",
    "      - items:  [{'id': id, 'kps': [x1,y1,...,xN,yN]}]\n",
    "    Chooses one annotation per image (the one with most visible kps).\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    idx = start_idx\n",
    "\n",
    "    # iterate over label files\n",
    "    for lbl_path in sorted(lbl_dir.glob(\"*.txt\")):\n",
    "        stem = lbl_path.stem\n",
    "\n",
    "        # find matching image (common extensions)\n",
    "        img_path = None\n",
    "        for ext in (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".PNG\"):\n",
    "            p = img_dir / f\"{stem}{ext}\"\n",
    "            if p.exists():\n",
    "                img_path = p\n",
    "                break\n",
    "        if img_path is None:\n",
    "            continue\n",
    "\n",
    "        with open(lbl_path) as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "        if not lines:\n",
    "            continue\n",
    "\n",
    "        # pick best line by # visible kps (v>0)\n",
    "        best = None\n",
    "        best_vis = -1\n",
    "        for ln in lines:\n",
    "            parts = ln.split()\n",
    "            # cls(1) + bbox(4) + (x,y,v)*num_kps\n",
    "            if len(parts) < 5 + 3 * num_kps:\n",
    "                continue\n",
    "            nums = list(map(float, parts))\n",
    "            kps = nums[5:]  # x1 y1 v1 x2 y2 v2 ...\n",
    "            vis = sum(1 for j in range(2, len(kps), 3) if kps[j] > 0)\n",
    "            if vis > best_vis:\n",
    "                best_vis = vis\n",
    "                best = nums\n",
    "\n",
    "        if best is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        k = best[5:]\n",
    "        xy = []\n",
    "        for j in range(0, 3 * num_kps, 3):\n",
    "            x_abs = k[j] * w\n",
    "            y_abs = k[j + 1] * h\n",
    "            xy.extend([float(x_abs), float(y_abs)])\n",
    "\n",
    "        out_path = out_img_dir / f\"{idx}.png\"\n",
    "        cv2.imwrite(str(out_path), img)\n",
    "\n",
    "        items.append({\"id\": idx, \"kps\": xy})\n",
    "        idx += 1\n",
    "\n",
    "    return items, idx\n",
    "\n",
    "# Build flat train/val\n",
    "train_items, nxt = parse_yolo_kps(rf_train_images, rf_train_labels, out_img_dir, start_idx=0, num_kps=12)\n",
    "with open(out_train_json, \"w\") as f:\n",
    "    json.dump(train_items, f)\n",
    "print(f\"FLAT train: {len(train_items)} items -> {out_train_json}, images in {out_img_dir}\")\n",
    "\n",
    "val_items, _ = parse_yolo_kps(rf_val_images, rf_val_labels, out_img_dir, start_idx=nxt, num_kps=12)\n",
    "with open(out_val_json, \"w\") as f:\n",
    "    json.dump(val_items, f)\n",
    "print(f\"FLAT val:   {len(val_items)} items -> {out_val_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7c8332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_images = Path(\"data_idx/images\")\n",
    "val_images   = Path(\"data_idx/images\")            # shared pool is fine\n",
    "train_ann    = Path(\"data_idx/data_train_flat.json\")\n",
    "val_ann      = Path(\"data_idx/data_val_flat.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be3cef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fallback] Created val set with 56 items at data_idx/data_val_flat.json\n"
     ]
    }
   ],
   "source": [
    "# Fallback: ensure val has items; if empty/missing, clone a small subset of train\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def _len_json(p):\n",
    "    try:\n",
    "        with open(p) as f:\n",
    "            data = json.load(f)\n",
    "        return len(data) if isinstance(data, list) else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "train_count = _len_json(train_ann)\n",
    "val_count   = _len_json(val_ann)\n",
    "\n",
    "if val_count == 0:\n",
    "    # make a small validation set from train (10% or at least 32, capped at train size)\n",
    "    with open(train_ann) as f:\n",
    "        train_items = json.load(f)\n",
    "    n = min(train_count, max(32, int(0.10 * train_count)))\n",
    "    with open(val_ann, \"w\") as f:\n",
    "        json.dump(train_items[:n], f)\n",
    "    print(f\"[fallback] Created val set with {n} items at {val_ann}\")\n",
    "else:\n",
    "    print(f\"val set OK: {val_count} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e490d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain datasets\n",
    "train_dataset = keyPointsDataset(str(train_images), str(train_ann))\n",
    "val_dataset   = keyPointsDataset(str(val_images), str(val_ann))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed3fe6",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0d52924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 12*2) # Replaces the last layer for 12 keypoints (x, y) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f5c95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656f29b",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46edb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6916324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/71], Loss: 15029.46875\n",
      "Epoch [1/20], Step [11/71], Loss: 16209.1044921875\n",
      "Epoch [1/20], Step [21/71], Loss: 15634.1923828125\n",
      "Epoch [1/20], Step [31/71], Loss: 14280.328125\n",
      "Epoch [1/20], Step [41/71], Loss: 15001.8076171875\n",
      "Epoch [1/20], Step [51/71], Loss: 14506.8134765625\n",
      "Epoch [1/20], Step [61/71], Loss: 13201.4990234375\n",
      "Epoch [1/20], Step [71/71], Loss: 12606.6240234375\n",
      "Epoch [2/20], Step [1/71], Loss: 13177.25390625\n",
      "Epoch [2/20], Step [11/71], Loss: 13829.2783203125\n",
      "Epoch [2/20], Step [21/71], Loss: 11905.25390625\n",
      "Epoch [2/20], Step [31/71], Loss: 11352.3408203125\n",
      "Epoch [2/20], Step [41/71], Loss: 11444.14453125\n",
      "Epoch [2/20], Step [51/71], Loss: 12211.4775390625\n",
      "Epoch [2/20], Step [61/71], Loss: 11875.9052734375\n",
      "Epoch [2/20], Step [71/71], Loss: 10319.080078125\n",
      "Epoch [3/20], Step [1/71], Loss: 11012.3330078125\n",
      "Epoch [3/20], Step [11/71], Loss: 10109.1435546875\n",
      "Epoch [3/20], Step [21/71], Loss: 10193.7841796875\n",
      "Epoch [3/20], Step [31/71], Loss: 9747.9267578125\n",
      "Epoch [3/20], Step [41/71], Loss: 8555.7802734375\n",
      "Epoch [3/20], Step [51/71], Loss: 8358.3037109375\n",
      "Epoch [3/20], Step [61/71], Loss: 8228.3955078125\n",
      "Epoch [3/20], Step [71/71], Loss: 7933.14794921875\n",
      "Epoch [4/20], Step [1/71], Loss: 7712.62255859375\n",
      "Epoch [4/20], Step [11/71], Loss: 7126.40771484375\n",
      "Epoch [4/20], Step [21/71], Loss: 6995.97900390625\n",
      "Epoch [4/20], Step [31/71], Loss: 6969.48974609375\n",
      "Epoch [4/20], Step [41/71], Loss: 6771.24560546875\n",
      "Epoch [4/20], Step [51/71], Loss: 6811.91796875\n",
      "Epoch [4/20], Step [61/71], Loss: 5972.07568359375\n",
      "Epoch [4/20], Step [71/71], Loss: 5407.61962890625\n",
      "Epoch [5/20], Step [1/71], Loss: 6205.28125\n",
      "Epoch [5/20], Step [11/71], Loss: 5045.81298828125\n",
      "Epoch [5/20], Step [21/71], Loss: 6011.71484375\n",
      "Epoch [5/20], Step [31/71], Loss: 6336.20849609375\n",
      "Epoch [5/20], Step [41/71], Loss: 4745.91650390625\n",
      "Epoch [5/20], Step [51/71], Loss: 4543.009765625\n",
      "Epoch [5/20], Step [61/71], Loss: 4279.05419921875\n",
      "Epoch [5/20], Step [71/71], Loss: 4302.546875\n",
      "Epoch [6/20], Step [1/71], Loss: 4253.38037109375\n",
      "Epoch [6/20], Step [11/71], Loss: 4100.70849609375\n",
      "Epoch [6/20], Step [21/71], Loss: 4367.85595703125\n",
      "Epoch [6/20], Step [31/71], Loss: 3677.558837890625\n",
      "Epoch [6/20], Step [41/71], Loss: 3587.186279296875\n",
      "Epoch [6/20], Step [51/71], Loss: 3824.342529296875\n",
      "Epoch [6/20], Step [61/71], Loss: 3421.763671875\n",
      "Epoch [6/20], Step [71/71], Loss: 3494.108154296875\n",
      "Epoch [7/20], Step [1/71], Loss: 3992.414794921875\n",
      "Epoch [7/20], Step [11/71], Loss: 3808.970703125\n",
      "Epoch [7/20], Step [21/71], Loss: 2753.260986328125\n",
      "Epoch [7/20], Step [31/71], Loss: 2753.6728515625\n",
      "Epoch [7/20], Step [41/71], Loss: 2755.696044921875\n",
      "Epoch [7/20], Step [51/71], Loss: 2393.571044921875\n",
      "Epoch [7/20], Step [61/71], Loss: 2441.305419921875\n",
      "Epoch [7/20], Step [71/71], Loss: 2549.891845703125\n",
      "Epoch [8/20], Step [1/71], Loss: 2232.385009765625\n",
      "Epoch [8/20], Step [11/71], Loss: 2453.2744140625\n",
      "Epoch [8/20], Step [21/71], Loss: 2036.1796875\n",
      "Epoch [8/20], Step [31/71], Loss: 2288.106201171875\n",
      "Epoch [8/20], Step [41/71], Loss: 1911.60546875\n",
      "Epoch [8/20], Step [51/71], Loss: 1960.60693359375\n",
      "Epoch [8/20], Step [61/71], Loss: 1710.1361083984375\n",
      "Epoch [8/20], Step [71/71], Loss: 1526.5228271484375\n",
      "Epoch [9/20], Step [1/71], Loss: 1588.833984375\n",
      "Epoch [9/20], Step [11/71], Loss: 1487.0687255859375\n",
      "Epoch [9/20], Step [21/71], Loss: 1827.9642333984375\n",
      "Epoch [9/20], Step [31/71], Loss: 1548.6396484375\n",
      "Epoch [9/20], Step [41/71], Loss: 1580.4237060546875\n",
      "Epoch [9/20], Step [51/71], Loss: 1577.4344482421875\n",
      "Epoch [9/20], Step [61/71], Loss: 1272.66845703125\n",
      "Epoch [9/20], Step [71/71], Loss: 1185.39501953125\n",
      "Epoch [10/20], Step [1/71], Loss: 1069.23291015625\n",
      "Epoch [10/20], Step [11/71], Loss: 1848.00048828125\n",
      "Epoch [10/20], Step [21/71], Loss: 891.4794921875\n",
      "Epoch [10/20], Step [31/71], Loss: 1009.4052124023438\n",
      "Epoch [10/20], Step [41/71], Loss: 999.263671875\n",
      "Epoch [10/20], Step [51/71], Loss: 916.361328125\n",
      "Epoch [10/20], Step [61/71], Loss: 857.6271362304688\n",
      "Epoch [10/20], Step [71/71], Loss: 908.8580932617188\n",
      "Epoch [11/20], Step [1/71], Loss: 981.5889282226562\n",
      "Epoch [11/20], Step [11/71], Loss: 816.1134643554688\n",
      "Epoch [11/20], Step [21/71], Loss: 1176.3800048828125\n",
      "Epoch [11/20], Step [31/71], Loss: 682.426513671875\n",
      "Epoch [11/20], Step [41/71], Loss: 614.010009765625\n",
      "Epoch [11/20], Step [51/71], Loss: 736.6675415039062\n",
      "Epoch [11/20], Step [61/71], Loss: 921.818603515625\n",
      "Epoch [11/20], Step [71/71], Loss: 613.4286499023438\n",
      "Epoch [12/20], Step [1/71], Loss: 1084.3575439453125\n",
      "Epoch [12/20], Step [11/71], Loss: 597.238525390625\n",
      "Epoch [12/20], Step [21/71], Loss: 465.4715881347656\n",
      "Epoch [12/20], Step [31/71], Loss: 1269.39892578125\n",
      "Epoch [12/20], Step [41/71], Loss: 1048.8375244140625\n",
      "Epoch [12/20], Step [51/71], Loss: 770.22705078125\n",
      "Epoch [12/20], Step [61/71], Loss: 670.3237915039062\n",
      "Epoch [12/20], Step [71/71], Loss: 409.9073181152344\n",
      "Epoch [13/20], Step [1/71], Loss: 419.7332763671875\n",
      "Epoch [13/20], Step [11/71], Loss: 706.2730102539062\n",
      "Epoch [13/20], Step [21/71], Loss: 310.9486083984375\n",
      "Epoch [13/20], Step [31/71], Loss: 281.7996520996094\n",
      "Epoch [13/20], Step [41/71], Loss: 713.9734497070312\n",
      "Epoch [13/20], Step [51/71], Loss: 702.0872192382812\n",
      "Epoch [13/20], Step [61/71], Loss: 297.9523010253906\n",
      "Epoch [13/20], Step [71/71], Loss: 337.4573059082031\n",
      "Epoch [14/20], Step [1/71], Loss: 225.8439483642578\n",
      "Epoch [14/20], Step [11/71], Loss: 242.454833984375\n",
      "Epoch [14/20], Step [21/71], Loss: 286.0669250488281\n",
      "Epoch [14/20], Step [31/71], Loss: 268.2785949707031\n",
      "Epoch [14/20], Step [41/71], Loss: 185.0287628173828\n",
      "Epoch [14/20], Step [51/71], Loss: 256.5317077636719\n",
      "Epoch [14/20], Step [61/71], Loss: 1156.66259765625\n",
      "Epoch [14/20], Step [71/71], Loss: 648.314697265625\n",
      "Epoch [15/20], Step [1/71], Loss: 646.639892578125\n",
      "Epoch [15/20], Step [11/71], Loss: 219.1947021484375\n",
      "Epoch [15/20], Step [21/71], Loss: 1706.8265380859375\n",
      "Epoch [15/20], Step [31/71], Loss: 561.3672485351562\n",
      "Epoch [15/20], Step [41/71], Loss: 814.7116088867188\n",
      "Epoch [15/20], Step [51/71], Loss: 199.2959442138672\n",
      "Epoch [15/20], Step [61/71], Loss: 291.8741455078125\n",
      "Epoch [15/20], Step [71/71], Loss: 633.7802124023438\n",
      "Epoch [16/20], Step [1/71], Loss: 168.10743713378906\n",
      "Epoch [16/20], Step [11/71], Loss: 164.08148193359375\n",
      "Epoch [16/20], Step [21/71], Loss: 591.173583984375\n",
      "Epoch [16/20], Step [31/71], Loss: 1054.3978271484375\n",
      "Epoch [16/20], Step [41/71], Loss: 196.3964385986328\n",
      "Epoch [16/20], Step [51/71], Loss: 135.03060913085938\n",
      "Epoch [16/20], Step [61/71], Loss: 539.4866333007812\n",
      "Epoch [16/20], Step [71/71], Loss: 449.6405944824219\n",
      "Epoch [17/20], Step [1/71], Loss: 145.83448791503906\n",
      "Epoch [17/20], Step [11/71], Loss: 119.29630279541016\n",
      "Epoch [17/20], Step [21/71], Loss: 219.9705352783203\n",
      "Epoch [17/20], Step [31/71], Loss: 142.83644104003906\n",
      "Epoch [17/20], Step [41/71], Loss: 98.48749542236328\n",
      "Epoch [17/20], Step [51/71], Loss: 923.3558959960938\n",
      "Epoch [17/20], Step [61/71], Loss: 719.7057495117188\n",
      "Epoch [17/20], Step [71/71], Loss: 1139.1478271484375\n",
      "Epoch [18/20], Step [1/71], Loss: 85.83170318603516\n",
      "Epoch [18/20], Step [11/71], Loss: 97.61978149414062\n",
      "Epoch [18/20], Step [21/71], Loss: 122.89181518554688\n",
      "Epoch [18/20], Step [31/71], Loss: 141.30496215820312\n",
      "Epoch [18/20], Step [41/71], Loss: 103.36994171142578\n",
      "Epoch [18/20], Step [51/71], Loss: 668.7642211914062\n",
      "Epoch [18/20], Step [61/71], Loss: 521.681640625\n",
      "Epoch [18/20], Step [71/71], Loss: 92.14949798583984\n",
      "Epoch [19/20], Step [1/71], Loss: 442.9718017578125\n",
      "Epoch [19/20], Step [11/71], Loss: 101.46578216552734\n",
      "Epoch [19/20], Step [21/71], Loss: 530.9585571289062\n",
      "Epoch [19/20], Step [31/71], Loss: 70.43226623535156\n",
      "Epoch [19/20], Step [41/71], Loss: 92.14349365234375\n",
      "Epoch [19/20], Step [51/71], Loss: 269.96453857421875\n",
      "Epoch [19/20], Step [61/71], Loss: 443.0948791503906\n",
      "Epoch [19/20], Step [71/71], Loss: 104.3322982788086\n",
      "Epoch [20/20], Step [1/71], Loss: 131.14425659179688\n",
      "Epoch [20/20], Step [11/71], Loss: 65.04328918457031\n",
      "Epoch [20/20], Step [21/71], Loss: 75.5577163696289\n",
      "Epoch [20/20], Step [31/71], Loss: 91.19315338134766\n",
      "Epoch [20/20], Step [41/71], Loss: 84.14740753173828\n",
      "Epoch [20/20], Step [51/71], Loss: 504.3907470703125\n",
      "Epoch [20/20], Step [61/71], Loss: 52.672149658203125\n",
      "Epoch [20/20], Step [71/71], Loss: 473.4154052734375\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, kps) in enumerate(train_loader):\n",
    "        imgs, kps = imgs.to(device), kps.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, kps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a042a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"keypoint_model2.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
